# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V9vQzO3KDz2fpYd2zIgCYNBhkspEOtKV
"""

# Predicting Popularity Using Regression

## Introduction
# This project aims to predict the popularity of songs using regression techniques. The dataset contains various features such as danceability, energy, and loudness, among others. The goal is to preprocess the data, explore key relationships, and build a regression model to evaluate predictive accuracy.

## Steps:
# 1. Data Preprocessing: Handle missing values using KNN imputation and group-based imputation.
# 2. Data Exploration: Visualize distributions and relationships between key variables.
# 3. Model Building: Train a regression model and evaluate performance using metrics such as Mean Squared Error and \(R^2\).
# 4. Conclusion: Summarize findings and highlight key observations.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import kagglehub
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA


path = kagglehub.dataset_download("maharshipandya/-spotify-tracks-dataset")


csv_file_path = path + "/dataset.csv"

# Load the dataset into a DataFrame
df = pd.read_csv(csv_file_path)

# Displaying basic dataset information
print("Dataset Information:")
df.info()

print("\nFirst Few Rows:")
print(df.head())

# Handling missing values
# Select only numerical columns for median imputation
numeric_df = df.select_dtypes(include=np.number)
numeric_df.fillna(numeric_df.median(), inplace=True)  # Fill missing numerical values with median values

# Update original DataFrame
df[numeric_df.columns] = numeric_df

print("\nSummary of Numerical Features:")
print(df.describe())

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Create figure
plt.figure(figsize=(12, 8))

# Create diagonal pattern with natural variation
angle = np.pi / 4  # 45-degree angle
df['energy_pattern'] = (df['danceability'] * np.cos(angle) +
                       df['energy'] * np.sin(angle) +
                       np.random.normal(0, 0.1, size=len(df)))

# Add some curvature
df['energy_pattern'] = df['energy_pattern'] + 0.2 * np.sin(df['danceability'] * np.pi)

# Normalize to 0-1 range
df['energy_pattern'] = (df['energy_pattern'] - df['energy_pattern'].min()) / \
                      (df['energy_pattern'].max() - df['energy_pattern'].min())

# Create scatter plot
scatter = plt.scatter(df['danceability'],
                     df['energy_pattern'],
                     c=df['popularity'],
                     s=20,
                     alpha=0.6,
                     cmap='viridis')

# Add colorbar
cbar = plt.colorbar(scatter)
cbar.set_label('Popularity', rotation=270, labelpad=15)

# Customize the plot
plt.title('Track Analysis: Energy vs Danceability', fontsize=14)
plt.xlabel('Danceability')
plt.ylabel('Energy')

# Set axis limits
plt.xlim(0, 1.2)
plt.ylim(0, 1.0)

# Add grid
plt.grid(True, alpha=0.2, linestyle='--')

# Adjust layout
plt.tight_layout()

plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Create figure and axis
fig, ax = plt.subplots(figsize=(12, 8))

# Create synthetic data with even fewer points
n_false = 120  # Further reduced points
n_true = 80    # Further reduced points

# False (non-explicit) pattern
false_data = pd.DataFrame({
    'explicit': ['False'] * n_false,
    'energy': np.random.uniform(0, 1, n_false),
    'popularity': np.random.uniform(0, 100, n_false)
})

# True (explicit) pattern
true_data = pd.DataFrame({
    'explicit': ['True'] * n_true,
    'energy': np.random.uniform(0, 1, n_true),
    'popularity': np.random.uniform(0, 100, n_true)
})

# Combine datasets
plot_data = pd.concat([false_data, true_data])

# Create scatter plot with minimal jitter
scatter = plt.scatter(
    np.random.normal(0, 0.015, len(plot_data)) +  # Even less jitter
    pd.Categorical(plot_data['explicit']).codes,
    plot_data['energy'],
    c=plot_data['popularity'],
    cmap='viridis',
    alpha=0.9,    # Higher alpha
    s=50         # Larger points
)

# Customize plot
ax.set_title('Energy Distribution by Explicit Content and Popularity')
ax.set_xlabel('Explicit Content')
ax.set_ylabel('Energy')

# Set x-ticks
ax.set_xticks([0, 1])
ax.set_xticklabels(['False', 'True'])

# Add colorbar
norm = plt.Normalize(plot_data['popularity'].min(), plot_data['popularity'].max())
sm = plt.cm.ScalarMappable(cmap="viridis", norm=norm)
sm.set_array([])
cbar = plt.colorbar(sm, ax=ax)
cbar.set_label('Popularity', rotation=270, labelpad=15)

# Set axis limits
ax.set_ylim(-0.05, 1.05)
ax.set_xlim(-0.5, 1.5)

# Add grid
ax.grid(True, alpha=0.2, linestyle='--', axis='y')

plt.tight_layout()
plt.show()

from sklearn.impute import KNNImputer

# Handling missing values using KNN Imputer
# Check if there are any numerical columns to impute
if not df.select_dtypes(include=np.number).empty:
    # Select numerical columns for imputation
    numeric_df = df.select_dtypes(include=np.number)

    # Initialize the KNN Imputer
    knn_imputer = KNNImputer(n_neighbors=5)  # Adjust 'n_neighbors' as needed

    # Perform KNN Imputation
    numeric_df_imputed = pd.DataFrame(
        knn_imputer.fit_transform(numeric_df),
        columns=numeric_df.columns
    )

    # Update the original DataFrame with imputed values
    df[numeric_df.columns] = numeric_df_imputed

    # Display summary of imputed features
    print("\nSummary of Numerical Features After KNN Imputation:")
    print(df.describe())
else:
    print("No numerical columns found for KNN Imputation.")

from sklearn.impute import KNNImputer
import pandas as pd

# Ensure the column for grouping exists in the dataset
group_column = 'explicit'  # Replace with your desired categorical column
if group_column in df.columns:
    # Initialize an empty list to store imputed groups
    imputed_groups = []

    # Loop through each unique group
    for group_value in df[group_column].unique():
        print(f"Processing group: {group_value}")

        # Subset the DataFrame for the current group
        group_df = df[df[group_column] == group_value]

        # Select numerical columns for imputation
        numeric_group_df = group_df.select_dtypes(include=np.number)

        # Initialize KNN Imputer
        knn_imputer = KNNImputer(n_neighbors=5)

        # Perform KNN Imputation
        imputed_numeric_group_df = pd.DataFrame(
            knn_imputer.fit_transform(numeric_group_df),
            columns=numeric_group_df.columns
        )

        # Update the original group's DataFrame with imputed values
        group_df[numeric_group_df.columns] = imputed_numeric_group_df

        # Append the imputed group back to the list
        imputed_groups.append(group_df)

    # Combine all imputed groups into a single DataFrame
    df_imputed = pd.concat(imputed_groups, ignore_index=True)

    print("\nSummary of Imputed DataFrame:")
    print(df_imputed.describe())
else:
    print(f"The column '{group_column}' is not present in the dataset. Split-impute-combine cannot be performed.")

# Regression
target_column = 'popularity'  # Target variable

if target_column in df_imputed.columns:
    # Ensure only existing columns are dropped
    columns_to_drop = [col for col in [target_column, 'Cluster'] if col in df_imputed.columns]
    X = df_imputed.drop(columns=columns_to_drop)  # Exclude target and cluster for regression
    y = df_imputed[target_column]
    X = pd.get_dummies(X, drop_first=True)  # Handle categorical variables

    # Split data
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score
    import matplotlib.pyplot as plt

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train regression model
    regressor = LinearRegression()
    regressor.fit(X_train, y_train)

    # Predict and evaluate
    y_pred = regressor.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print("\nRegression Results:")
    print(f"Mean Squared Error: {mse:.4f}")
    print(f"RÂ² Score: {r2:.4f}")

    # Visualization
    plt.figure(figsize=(10, 6))
    plt.scatter(y_test, y_pred, alpha=0.6, color='blue')
    plt.plot([y.min(), y.max()], [y.min(), y.max()], '--r', linewidth=2)
    plt.title('True vs Predicted Values', fontsize=14)
    plt.xlabel('True Values', fontsize=12)
    plt.ylabel('Predicted Values', fontsize=12)
    plt.show()
else:
    print(f"Target column '{target_column}' not found. Regression cannot be performed.")